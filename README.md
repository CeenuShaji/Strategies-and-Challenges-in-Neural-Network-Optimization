# Homework 4 Part 2

**Due: Tuesday, December 6 @ 11:59 PM**

Problem 1: **Overcoming Neural Network Training Challenges**  
Identifies vanishing gradients, insufficient data, slow training, and overfitting as major neural network training challenges, suggesting solutions like batch normalization, transfer learning, momentum gradient descent, and regularization.

Problem 2: **Activation Functions and Neural Representations**  
Assesses neural networks with linear and hard-limit activation functions for their ability to represent functions like polynomials and piecewise constant functions, concluding their effectiveness varies by case.

Problem 3: **Momentum in Neural Network Training**  
Discusses the role of momentum in avoiding local minima and speeding up training, cautioning that too high a momentum value can lead to overshooting the global minimum.

Problem 4: **Standard vs. Nesterov Momentum**  
Compares standard momentum optimization to Nesterov Accelerated Gradient, highlighting Nesterov's preemptive gradient evaluation as a key advantage for faster convergence.

Problem 5: **Weight Updates and Hidden Layer Training in MLPs**  
Explains the process of updating weights using the chain rule and highlights the vanishing/exploding gradient problem as a significant challenge in training MLP hidden layers.

Problem 6: **Learning Curves in MLP Training**  
Involves analyzing learning curves from mini-batch, batch, and online learning in MLP training to identify which curve corresponds to each learning type.

Problem 7: **Validating Statements on Neural Network Initialization and CNNs**  
Evaluates statements about weight initialization, the function of pooling layers in CNNs, and the nature of filters in early CNN layers, determining the validity of each.

Problem 8: **Output Layer Configuration in MLP for Multi-Class Classification**  
Discusses the appropriate number of units and activation functions for the output layer in a 10-label classification problem using MLP, comparing the implications of using 1 unit versus 10 units.

Problem 9: **Backpropagation Calculation Example**  
Requests a detailed backpropagation calculation for a given network structure and initial weights, assuming a linear activation function and a specific learning rate, to find updated weight values after one iteration.

**This is a individuak assignment.**

Find the assignment description in the file "Homework 4 Part 2.ipynb".

The complete rubric for this assignment can be found in the Canvas page under [Assignments -> Homework 4 Part 2](https://ufl.instructure.com/courses/464118/assignments/5434151).
